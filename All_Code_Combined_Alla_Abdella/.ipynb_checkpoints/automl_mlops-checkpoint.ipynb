{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azureml-sdk\n",
    "!pip install azureml-train-automl\n",
    "!pip install pandas\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = '{subscription-id}'\n",
    "resource_group  = '{resource-group}'\n",
    "workspace_name  = '{workspace-name}'\n",
    "experiment_name = '{experiment-name}'\n",
    "project_folder = './contoso-aml'\n",
    "cluster_name = '{aml-compute-cluster-name}'\n",
    "aks_cluster_name = '{aks-cluster-name}'\n",
    "aks_service_name ='{aks_service_name}'\n",
    "\n",
    "# Working directory\n",
    "if not os.path.exists(project_folder):\n",
    "    os.makedirs(project_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import open source Python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import random\n",
    "import re\n",
    "import lightgbm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import csv\n",
    "from shutil import copy2\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "sns.set(color_codes='True')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Azure Machine Learning Python SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.compute import AksCompute, ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.conda_dependencies import CondaDependencies \n",
    "from azureml.core.datastore import Datastore\n",
    "from azureml.core.dataset import Dataset\n",
    "from azureml.core.webservice import Webservice, AksWebservice, AciWebservice\n",
    "from azureml.core.image import ContainerImage, Image\n",
    "from azureml.core.model import Model\n",
    "from azureml.core.runconfig import DataReferenceConfiguration, RunConfiguration\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.train.automl.run import AutoMLRun\n",
    "from azureml.widgets import RunDetails\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.pipeline.core import Pipeline, PipelineData, PublishedPipeline, PipelineRun, Schedule, TrainingOutput\n",
    "from azureml.pipeline.core.graph import PipelineParameter\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.train.automl import AutoMLConfig, AutoMLStep\n",
    "from azureml.train.automl.automlexplainer import retrieve_model_explanation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Azure Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library configuration succeeded\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "    ws.write_config()\n",
    "    ws = Workspace.from_config()\n",
    "    print('Library configuration succeeded')\n",
    "except:\n",
    "    print('Workspace not found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Azure AutoML Workspace Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SDK version</th>\n",
       "      <td>1.0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Subscription ID</th>\n",
       "      <td>2a779d6f-0806-4359-a6e8-f1fd57bb5dd7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Workspace</th>\n",
       "      <td>contoso-workspace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experiment</th>\n",
       "      <td>contoso-aml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Resource Group</th>\n",
       "      <td>devintersection-2018-aml-demo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Location</th>\n",
       "      <td>westus2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Project Directory</th>\n",
       "      <td>./contoso-aml</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       \n",
       "SDK version        1.0.43                              \n",
       "Subscription ID    2a779d6f-0806-4359-a6e8-f1fd57bb5dd7\n",
       "Workspace          contoso-workspace                   \n",
       "Experiment         contoso-aml                         \n",
       "Resource Group     devintersection-2018-aml-demo       \n",
       "Location           westus2                             \n",
       "Project Directory  ./contoso-aml                       "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['SDK version'] = azureml.core.VERSION\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace'] = ws.name\n",
    "output['Experiment'] = experiment.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Project Directory'] = project_folder\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.DataFrame(data=output, index=['']).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get DataStore and list DataSets (Azure Storage, Azure Data Lake, Azure SQL and more)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vehicle_testing\n",
      "vehicle_training_simple\n",
      "vehicle_training\n",
      "vehicle_data\n"
     ]
    }
   ],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "for dataset in Dataset.list(ws):\n",
    "    print(dataset.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = ws.datasets['vehicle_training']\n",
    "test_data = ws.datasets['vehicle_data']\n",
    "\n",
    "train_df = train_data.to_pandas_dataframe()\n",
    "test_df = test_data.to_pandas_dataframe() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_list = ['Survival_In_Days']\n",
    "df = train_df[col_list]\n",
    "sns.distplot(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "col_list = ['Survival_In_Days', 'Trip_Length_Sigma', 'Trips_Per_Day_Sigma']\n",
    "df = train_df[col_list]\n",
    "sns.pairplot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create get_data.py for Automate Machine Learning to use training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $project_folder/get_data.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from azureml.core import Workspace, Datastore, Dataset, Run\n",
    "\n",
    "def get_data():\n",
    "    run = Run.get_context()\n",
    "    workspace = run.experiment.workspace\n",
    "    dataset = Dataset.get(workspace=workspace, name='vehicle_training')\n",
    "    \n",
    "    # Get dataset by name\n",
    "    train_data = dataset.to_pandas_dataframe()\n",
    "    \n",
    "    X = train_data.iloc[:,1:74]\n",
    "    Y = train_data.iloc[:,0].values\n",
    "\n",
    "    return { \"X\" : X.values, \"y\" : Y.flatten() }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create AML Compute Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing compute target.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target.')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D12_V2', max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "    compute_target.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../mlsolutions/workplacesafety/resources/compute.png'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Automated Machine Learning Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" width=\"850\" src=\"https://raw.githubusercontent.com/chrislauren/amlallhands/master/automl%20overview.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfiguration(framework=\"python\")\n",
    "run_config.target = compute_target\n",
    "run_config.environment.docker.enabled = True\n",
    "\n",
    "automl_config = AutoMLConfig(task = 'regression',\n",
    "                             iterations = 25,\n",
    "                             iteration_timeout_minutes = 5, \n",
    "                             max_cores_per_iteration = 10,\n",
    "                             preprocess = True,\n",
    "                             primary_metric = 'normalized_root_mean_squared_error',\n",
    "                             n_cross_validations = 2,\n",
    "                             debug_log = 'automl.log',\n",
    "                             verbosity = logging.DEBUG,\n",
    "                             data_script = project_folder + \"/get_data.py\",\n",
    "                             run_configuration = run_config,\n",
    "                             #compute_target = compute_target,\n",
    "                             #blacklist_models = \"\",\n",
    "                             path = project_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run our Experiment on AML Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run = experiment.submit(automl_config, show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Automated ML Run Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db4ecd7e03684db195febef0359cf15e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_AutoMLWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': False, 'log_level': 'INFO', 's…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RunDetails(remote_run).show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show best run and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = remote_run.get_output()\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publish best model to Azure Machine Learning Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the model for deployment\n",
    "model = best_run.register_model(model_name='battery_failure_predictor', \n",
    "                                model_path='outputs/model.pkl', \n",
    "                                tags = {'area': \"auto\", 'type': \"regression\"}) \n",
    "print(\"Model name: \" + model.name, \"Model version: \" + str(model.version), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Scoring File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "import pickle\n",
    "import json\n",
    "import azureml.train.automl\n",
    "import numpy as np\n",
    "\n",
    "from azureml.core.model import Model\n",
    "from inference_schema.schema_decorators import input_schema, output_schema\n",
    "from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "input_sample = np.array([['Centre_Val_de_Loire', 'MidWest', 18.12363, 6.041211, 4.022051, 1.0055129999999999, 250, 'M10', 'Y2013', 0.8890518, 'True', -2.081553, -7.993295, 10.80828, -12.12702, -0.6662728, -10.65882, 0.6769765999999999, -14.394279999999998, -2.4150240000000003, -7.103573, 7.1126830000000005, -6.97204, 1.0697889999999999, -17.84025, 7.2463169999999995, -20.07207, -0.3555982, -10.730080000000001, -1.621019, -9.052511, -3.151685, -12.38087, -2.213263, -8.923146000000001, 11.35322, -14.51881, 4.450673, -9.823507000000001, -2.996298, -9.121293, -0.5239272, -24.48196, -2.7362900000000003, -10.16089, -1.014979, -7.749569999999999, -1.9338509999999998, -22.129839999999998, -0.3227415, -6.803357000000001, 0.8542936, -14.50496, -0.44740240000000003, -13.992920000000002, -1.34541, -14.18944, -1.526483, -11.79697, -0.9755153, -17.62779, 3.219577, -8.458607, -0.3016018, -9.000634, 3.9828300000000003, -12.40082, 3.7529589999999997, -16.78719, 3.178833, -9.794724, 2.012089, -16.29766]])\n",
    "output_sample = np.array([0])\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # this name is model.id of model that we want to deploy\n",
    "    model_path = Model.get_model_path(model_name = 'battery_failure_predictor')\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "@input_schema('data', NumpyParameterType(input_sample))\n",
    "@output_schema(NumpyParameterType(output_sample))\n",
    "def run(data):\n",
    "    try:\n",
    "        result = model.predict(data)\n",
    "        print(result)\n",
    "    except Exception as e:\n",
    "        result = str(e)\n",
    "        print('Exception Ocurred')\n",
    "        print(e)\n",
    "        return {\"error\": result}\n",
    "    return {\"result\":result.tolist()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Environment Dependency File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Conda environment specification. The dependencies defined in this file will\r\n",
      "# be automatically provisioned for runs with userManagedDependencies=False.\r\n",
      "\n",
      "# Details about the Conda environment file format:\r\n",
      "# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually\r\n",
      "\n",
      "name: project_environment\n",
      "dependencies:\n",
      "  # The python interpreter version.\r\n",
      "  # Currently Azure ML only supports 3.5.2 and later.\r\n",
      "- python=3.6.2\n",
      "\n",
      "- pip:\n",
      "  - azureml-sdk[notebooks,automl]\n",
      "  - inference-schema\n",
      "- numpy\n",
      "- scikit-learn\n",
      "channels:\n",
      "- conda-forge\n",
      "\n"
     ]
    }
   ],
   "source": [
    "myenv = CondaDependencies.create(conda_packages=['numpy','scikit-learn'],pip_packages=['azureml-sdk[notebooks,automl]','inference-schema'])\n",
    "print(myenv.serialize_to_string())\n",
    "\n",
    "with open(\"myenv.yml\",\"w\") as f:\n",
    "    f.write(myenv.serialize_to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.name, model.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create/connect to the Kubernetes compute cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the default configuration (can also provide parameters to customize)\n",
    "prov_config = AksCompute.provisioning_configuration(location='eastus2')\n",
    "\n",
    "# Create the cluster\n",
    "aks_target = ComputeTarget.create(workspace = ws, \n",
    "                          name = aks_cluster_name, \n",
    "                          provisioning_configuration = prov_config)\n",
    "\n",
    "aks_target.wait_for_completion(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploy the model to Kubernetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_config = InferenceConfig(runtime= \"python\", entry_script=\"score.py\", conda_file=\"myenv.yml\")\n",
    "\n",
    "deployment_config = AksWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1)\n",
    "\n",
    "service = Model.deploy(ws, aks_service_name, [model], inference_config, deployment_config, aks_target)\n",
    "\n",
    "service.wait_for_deployment(show_output = True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the deployed webservice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "aks_service = Webservice(ws,aks_service_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'result': [1552.1351300157253, 1401.3791786358465, 1465.7011829586686, 1727.9514716303725, 1738.022517834776, 1121.8909903529561, 2230.1036098625264, 1289.8034144180742, 1750.9422334858118, 1906.4357235894115]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import csv\n",
    "import requests\n",
    "\n",
    "# prepare the test data\n",
    "test__df = test_df.drop(columns=[\"Car_ID\", \"Battery_Age\"])\n",
    "sample = test__df.values.tolist()\n",
    "\n",
    "headers = {'Content-Type':'application/json'}\n",
    "\n",
    "if aks_service.auth_enabled:\n",
    "    headers['Authorization'] = 'Bearer '+aks_service.get_keys()[0]\n",
    "\n",
    "test_sample = json.dumps({'data': sample})\n",
    "response = requests.post(aks_service.scoring_uri, data=test_sample, headers=headers)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = response.json()['result']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the prediction and calculate which batteries need testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of cars that need batteries testing: 10.0%\n"
     ]
    }
   ],
   "source": [
    "result_columns = ['Car_ID','Predicted_Days_Remaining', 'Index']\n",
    "result_df = pd.DataFrame(columns=result_columns)\n",
    "\n",
    "for i in range(len(results)):\n",
    "    car_ID = test_df['Car_ID'].loc[i]\n",
    "    days_remaining = results[i] - test_df['Battery_Age'].loc[i]\n",
    "    if days_remaining < 31:\n",
    "        car_values = [car_ID, days_remaining, i]\n",
    "        result_df.loc[result_df.shape[0],:] = car_values\n",
    "\n",
    "print('Percentage of cars that need batteries testing: {}%'.format(result_df.shape[0] / len(results) * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create models tailored for each model and year of car using Azure Machine Learning Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-use our experiment configuration\n",
    "input_data = DataReference(datastore=ds, \n",
    "                           data_reference_name='training_data',\n",
    "                           path_on_datastore='data',\n",
    "                           mode='download',\n",
    "                           path_on_compute='/tmp/azureml_runs',\n",
    "                           overwrite=True\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over models and years of cars\n",
    "car_models = ['ContosoXL','ContosoML']\n",
    "car_years = ['2013','2014','2015','2016','2017']\n",
    "\n",
    "steps = []\n",
    "current = None\n",
    "ds = ws.get_default_datastore()\n",
    "\n",
    "# Build a model for every category\n",
    "for i in car_models:\n",
    "    for y in car_years:\n",
    "                  \n",
    "        automl_config = AutoMLConfig('regression',\n",
    "                                     iterations = 10,\n",
    "                                     iteration_timeout_minutes = 5, \n",
    "                                     max_cores_per_iteration = 10,\n",
    "                                     preprocess = True,\n",
    "                                     primary_metric = 'normalized_root_mean_squared_error',\n",
    "                                     n_cross_validations = 2,                                     \n",
    "                                     debug_log = 'automl.log',                                   \n",
    "                                     verbosity = logging.DEBUG,\n",
    "                                     data_script = '{}/get_data_c_{}_{}.py'.format(project_folder, i, y),\n",
    "                                     run_configuration = run_config,\n",
    "                                     compute_target = compute_target,\n",
    "                                     path = project_folder)\n",
    "\n",
    "        # AutoML action\n",
    "        automl_step = AutoMLStep(name='automl_module__category_{}_{}'.format(i, y),\n",
    "                                 automl_config=automl_config,\n",
    "                                 inputs=[input_data],\n",
    "                                 outputs=[metrics_data, model_data],\n",
    "                                 allow_reuse=False)\n",
    "\n",
    "        # These are the two outputs from AutoML\n",
    "        metrics_data = PipelineData(name='metrics_data_category_{}_{}'.format(i, y),\n",
    "                                    datastore=ds,\n",
    "                                    pipeline_output_name='metrics_output_category_{}_{}'.format(i, y),\n",
    "                                    training_output=TrainingOutput(type='Metrics'))\n",
    "\n",
    "        model_data = PipelineData(name='model_data_category_{}_{}'.format(i, y),\n",
    "                                  datastore=ds,\n",
    "                                  pipeline_output_name='best_model_output_category_{}_{}'.format(i, y),\n",
    "                                  training_output=TrainingOutput(type='Model'))\n",
    "        \n",
    "        # register the model afterwards\n",
    "        register_step = PythonScriptStep(name='register_category_{}_{}'.format(i, y),\n",
    "                                         script_name='register.py',\n",
    "                                         compute_target=compute_target,\n",
    "                                         source_directory=project_folder,\n",
    "                                         arguments=['--model_name', '{}_{}_battery_failure_predictor'.format(i, y), '--model_path', model_data],\n",
    "                                         inputs=[model_data],\n",
    "                                         # These are the two outputs from AutoML\n",
    "                                         allow_reuse=False)\n",
    "\n",
    "        # And chain them together so they run sequentially\n",
    "        if current:\n",
    "            automl_step.run_after(current)\n",
    "\n",
    "        current = register_step\n",
    "\n",
    "        steps.append(automl_step)\n",
    "        steps.append(register_step)\n",
    "\n",
    "        pipeline = Pipeline(description='Generate recommendation models',\n",
    "                        workspace=ws,\n",
    "                        steps=steps)\n",
    "\n",
    "        pipeline.validate()\n",
    "\n",
    "        # Once published, we can invoke on demand via the SDK or via a REST endpoint\n",
    "        published_pipeline = pipeline.publish(name='contoso-{}_{}'.format(i, y))\n",
    "        steps = []\n",
    "        current = None\n",
    "        \n",
    "        # Submit a run for the newly created pipeline\n",
    "        #published_pipeline.submit(ws, published_pipeline.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
