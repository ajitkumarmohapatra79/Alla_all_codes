{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "import azureml.core\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core import Workspace\n",
    "from azureml.core.model import Model\n",
    "from azureml.core import Experiment\n",
    "from azureml.core.webservice import Webservice\n",
    "from azureml.core.image import ContainerImage\n",
    "from azureml.core.webservice import AciWebservice\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "import collections\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle as pkl\n",
    "import logging\n",
    "import azureml.dataprep as dprep \n",
    "from azureml.core import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install -U tensorflow-gpu==1.14.0 tensorflow==estimator==1.14.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df = pd.read_csv('data/AI_ML_OrderExport_0812_v1.csv')\n",
    "#test_df= pd.read_csv('data/AI_ML_OrderExport_0801_0810.csv')\n",
    "\n",
    "data_df = pd.read_csv('Training_AI_ML_OrderExport_0701_0930_v2.csv')\n",
    "test_df= pd.read_csv('Test_AI_ML_OrderExport_1001_1014_v2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_df.isnull().sum()\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data=data_df[['TOTAL_PRICE','THIRD_PARTY_ID_SCORE','IS_EXISTING_CUSTOMER','NUM_PORTIN','FIRST_PARTY_ID_SCORE','ONETIMECHARGE','INSTALLMENT_AMOUNT','DEVICE_AT_HOME','FRAUDNET_SCORE','NUM_BYOD','IDA_RESULT','ALLOWED_LINES','MONTHLYRECURRINGCHARGE','EXTERNAL_CREDIT_CHECK_DONE','SALES_CHANNEL','EXTERNAL_APPROVED_LINES','APPROVED_LINES','ACNT_BILL_LNAME_MATCHES','ACNT_SHIP_FNAME_MATCHES','ACNT_SHIP_LNAME_MATCHES','SHIP_BILL_FNAME_MATCHES','BILL_SHIP_ADDR_MATCHES','MAKE1','MAKE2','MAKE3','MAKE4','MAKE5','MODEL1','MODEL2','MODEL3','MODEL4','MODEL5','PRICE1','PRICE2','PRICE3','PRICE4','PRICE5','HOUR_OF_DAY','FINAL_RESULT']]\n",
    "test_data=test_df[['TOTAL_PRICE','THIRD_PARTY_ID_SCORE','IS_EXISTING_CUSTOMER','NUM_PORTIN','FIRST_PARTY_ID_SCORE','ONETIMECHARGE','INSTALLMENT_AMOUNT','DEVICE_AT_HOME','FRAUDNET_SCORE','NUM_BYOD','IDA_RESULT','ALLOWED_LINES','MONTHLYRECURRINGCHARGE','EXTERNAL_CREDIT_CHECK_DONE','SALES_CHANNEL','EXTERNAL_APPROVED_LINES','APPROVED_LINES','ACNT_BILL_LNAME_MATCHES','ACNT_SHIP_FNAME_MATCHES','ACNT_SHIP_LNAME_MATCHES','SHIP_BILL_FNAME_MATCHES','BILL_SHIP_ADDR_MATCHES','MAKE1','MAKE2','MAKE3','MAKE4','MAKE5','MODEL1','MODEL2','MODEL3','MODEL4','MODEL5','PRICE1','PRICE2','PRICE3','PRICE4','PRICE5','HOUR_OF_DAY','FINAL_RESULT']]\n",
    "\n",
    "#data=data_df[['TOTAL_PRICE','THIRD_PARTY_ID_SCORE','IS_EXISTING_CUSTOMER','NUM_PORTIN','FIRST_PARTY_ID_SCORE','ONETIMECHARGE','INSTALLMENT_AMOUNT','DEVICE_AT_HOME','FRAUDNET_SCORE','NUM_BYOD','IDA_RESULT','MONTHLYRECURRINGCHARGE','EXTERNAL_CREDIT_CHECK_DONE','SALES_CHANNEL','ACNT_BILL_LNAME_MATCHES','ACNT_SHIP_FNAME_MATCHES','ACNT_SHIP_LNAME_MATCHES','SHIP_BILL_FNAME_MATCHES','BILL_SHIP_ADDR_MATCHES','MODEL1','MODEL2','MODEL3','MODEL4','MODEL5','ACTIVE_WATCH_RESULT']]\n",
    "#test_data=test_df[['TOTAL_PRICE','THIRD_PARTY_ID_SCORE','IS_EXISTING_CUSTOMER','NUM_PORTIN','FIRST_PARTY_ID_SCORE','ONETIMECHARGE','INSTALLMENT_AMOUNT','DEVICE_AT_HOME','FRAUDNET_SCORE','NUM_BYOD','IDA_RESULT','MONTHLYRECURRINGCHARGE','EXTERNAL_CREDIT_CHECK_DONE','SALES_CHANNEL','ACNT_BILL_LNAME_MATCHES','ACNT_SHIP_FNAME_MATCHES','ACNT_SHIP_LNAME_MATCHES','SHIP_BILL_FNAME_MATCHES','BILL_SHIP_ADDR_MATCHES','MODEL1','MODEL2','MODEL3','MODEL4','MODEL5','ACTIVE_WATCH_RESULT']]\n",
    "\n",
    "#data=data_df[['TOTAL_PRICE','THIRD_PARTY_ID_SCORE','IS_EXISTING_CUSTOMER','NUM_PORTIN','FIRST_PARTY_ID_SCORE','ONETIMECHARGE','INSTALLMENT_AMOUNT','DEVICE_AT_HOME','FRAUDNET_SCORE','NUM_BYOD','IDA_RESULT','EXTERNAL_CREDIT_CHECK_DONE','SALES_CHANNEL','MODEL1','MODEL2','MODEL3','MODEL4','MODEL5','ACTIVE_WATCH_RESULT']]\n",
    "#test_data=test_df[['TOTAL_PRICE','THIRD_PARTY_ID_SCORE','IS_EXISTING_CUSTOMER','NUM_PORTIN','FIRST_PARTY_ID_SCORE','ONETIMECHARGE','INSTALLMENT_AMOUNT','DEVICE_AT_HOME','FRAUDNET_SCORE','NUM_BYOD','IDA_RESULT','EXTERNAL_CREDIT_CHECK_DONE','SALES_CHANNEL','MODEL1','MODEL2','MODEL3','MODEL4','MODEL5','ACTIVE_WATCH_RESULT']]\n",
    "\n",
    "# data=data_df[['TOTAL_PRICE','THIRD_PARTY_ID_SCORE','IS_EXISTING_CUSTOMER','NUM_PORTIN','FIRST_PARTY_ID_SCORE','ONETIMECHARGE','INSTALLMENT_AMOUNT','DEVICE_AT_HOME','FRAUDNET_SCORE','NUM_BYOD','IDA_RESULT','EXTERNAL_CREDIT_CHECK_DONE','SALES_CHANNEL','MODEL1','MODEL2','MODEL3','MODEL4','MODEL5','ACTIVE_WATCH_RESULT']]\n",
    "# test_data=test_df[['TOTAL_PRICE','THIRD_PARTY_ID_SCORE','IS_EXISTING_CUSTOMER','NUM_PORTIN','FIRST_PARTY_ID_SCORE','ONETIMECHARGE','INSTALLMENT_AMOUNT','DEVICE_AT_HOME','FRAUDNET_SCORE','NUM_BYOD','IDA_RESULT','EXTERNAL_CREDIT_CHECK_DONE','SALES_CHANNEL','MODEL1','MODEL2','MODEL3','MODEL4','MODEL5','ACTIVE_WATCH_RESULT']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['MODEL2'].fillna('NA',inplace=True)\n",
    "data['MODEL3'].fillna('NA',inplace=True)\n",
    "data['MODEL4'].fillna('NA',inplace=True)\n",
    "data['MODEL5'].fillna('NA',inplace=True)\n",
    "\n",
    "\n",
    "test_data['MODEL2'].fillna('NA',inplace=True)\n",
    "test_data['MODEL3'].fillna('NA',inplace=True)\n",
    "test_data['MODEL4'].fillna('NA',inplace=True)\n",
    "test_data['MODEL5'].fillna('NA',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.fillna('NA',inplace=True)\n",
    "data.fillna('NaN',inplace=True)\n",
    "data.fillna(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_color(row):\n",
    "    color = row['FINAL_RESULT']\n",
    "#     print(color)\n",
    "    if color == 'GREEN':\n",
    "        return 0\n",
    "    elif color == 'YELLOW':\n",
    "        return 1\n",
    "    elif color == 'RED':\n",
    "        return 2\n",
    "    else:\n",
    "        print('We Received UnExpected Color:',color)\n",
    "        raise ValueError('invalid result:', color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label'] = data.apply(label_color, axis=1)\n",
    "data.drop(columns=['FINAL_RESULT'], inplace=True)\n",
    "\n",
    "test_data['label'] = test_data.apply(label_color, axis=1)\n",
    "test_data.drop(columns=['FINAL_RESULT'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collections.Counter(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_df = data[data['label'] == 0]\n",
    "yellow_df = data[data['label'] == 1]\n",
    "red_df = data[data['label'] == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_under_df = green_df.sample(11176)\n",
    "yellow_over_df= yellow_df.sample(11176, replace=True)\n",
    "#>>> df.sample(frac=0.5, replace=True, random_state=1)\n",
    "# return a random sample of row from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data_df = pd.concat([green_under_df, yellow_over_df, red_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = balanced_data_df.drop(columns=['label'])\n",
    "Y_train = balanced_data_df['label']\n",
    "\n",
    "X_test = test_data.drop(columns=['label'])\n",
    "Y_test = test_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np.array(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data_df.to_csv('data/risk_train.csv',index=False)\n",
    "test_data.to_csv('data/risk_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# choose a name for experiment\n",
    "experiment_name = 'risk-remote'\n",
    "# project folder\n",
    "project_folder = './ml_experiment/risk-remote'\n",
    " \n",
    "experiment = Experiment(ws, experiment_name)\n",
    " \n",
    "output = {}\n",
    "output['SDK version'] = azureml.core.VERSION\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace Name'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Project Directory'] = project_folder\n",
    "output['Experiment Name'] = experiment.name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dflow = dprep.read_csv('data/risk_train.csv', infer_column_types=True)\n",
    "train_dflow.get_profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dflow = dprep.read_csv('data/risk_test.csv', infer_column_types=True)\n",
    "test_dflow.get_profile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df = train_dflow.drop_columns(columns=['label'])\n",
    "y_df = train_dflow.keep_columns(columns=['label'], validate_column_exists=True)\n",
    "X_test_df = test_dflow.drop_columns(columns=['label'])\n",
    "y_test_df = test_dflow.keep_columns(columns=['label'], validate_column_exists=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "\n",
    "cts = ws.compute_targets\n",
    "amlcompute_cluster_name = \"XM-Heavy-Load\"\n",
    "compute_target = cts[amlcompute_cluster_name]\n",
    "\n",
    "\n",
    "\n",
    "found = False\n",
    "\n",
    "# # Check if this compute target already exists in the workspace.\n",
    "\n",
    "cts = ws.compute_targets\n",
    "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'AmlCompute':\n",
    "    found = True\n",
    "    print('Found existing compute target.')\n",
    "    compute_target = cts[amlcompute_cluster_name]\n",
    "\n",
    "if not found:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\", # for GPU, use \"STANDARD_NC6\"\n",
    "                                                                vm_priority = 'lowpriority',\n",
    "                                                                min_nodes = 0, max_nodes = 6)\n",
    "\n",
    "    # Create the cluster.\\n\",\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
    "\n",
    "#     # Can poll for a minimum number of nodes and for a specific timeout.\n",
    "#     # If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "#     compute_target.wait_for_completion(show_output = True, min_node_count = None, timeout_in_minutes = 20)\n",
    "\n",
    "#      # For a more detailed view of current AmlCompute status, use get_status()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "import pkg_resources\n",
    "\n",
    "# create a new RunConfig object\n",
    "conda_run_config = RunConfiguration(framework=\"python\")\n",
    "\n",
    "# Set compute target to AmlCompute\n",
    "conda_run_config.target = compute_target\n",
    "conda_run_config.environment.docker.enabled = True\n",
    "conda_run_config.environment.docker.base_image = azureml.core.runconfig.DEFAULT_CPU_IMAGE\n",
    "\n",
    "# dprep_dependency = 'azureml-dataprep==' + pkg_resources.get_distribution(\"azureml-dataprep\").version\n",
    "\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-sdk[automl]', 'psutil'], conda_packages=['numpy','py-xgboost<=0.80'])\n",
    "# cd = CondaDependencies.create(pip_packages=['azureml-sdk[automl]', dprep_dependency], conda_packages=['numpy','py-xgboost<=0.80'])\n",
    "conda_run_config.environment.python.conda_dependencies = cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump X and Y to CSVs.\n",
    "X_df.to_pandas_dataframe().to_csv('data/X_train.csv', index=False)\n",
    "y_df.to_pandas_dataframe().to_csv('data/Y_train.csv', index=False)\n",
    "\n",
    "# Dump X and Y to CSVs.\n",
    "X_test_df.to_pandas_dataframe().to_csv('data/X_test.csv', index=False)\n",
    "#y_test_df.to_pandas_dataframe().to_csv('data/risk/Y_test.csv', index=False)\n",
    "y_test_df.to_pandas_dataframe().to_csv('data/Y_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "ds.upload(src_dir='./data', target_path='data/', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Dataset.Tabular.from_delimited_files(path=ds.path('data/risk/X_train.csv'))\n",
    "y = Dataset.Tabular.from_delimited_files(path=ds.path('data/risk/Y_train.csv'))\n",
    "\n",
    "X_valid = Dataset.Tabular.from_delimited_files(path=ds.path('data/risk/X_test.csv'))\n",
    "y_valid = Dataset.Tabular.from_delimited_files(path=ds.path('data/risk/Y_test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_config = AutoMLConfig(task = 'classification',\n",
    "                             model_explainability=True,\n",
    "                             primary_metric = 'AUC_weighted',\n",
    "                             debug_log='automl_errors.log',\n",
    "                             path = project_folder, #For external compute\n",
    "                             run_configuration=conda_run_config, #For external compute\n",
    "                             preprocess=True,\n",
    "                             X = X, \n",
    "                             y = y,\n",
    "                             X_valid=X_valid,\n",
    "                             y_valid=y_valid,\n",
    "                             iteration_timeout_minutes=10,\n",
    "                             experiment_timeout_minutes=200,\n",
    "                             max_concurrent_iterations=10,\n",
    "                             max_cores_per_iteration=4,\n",
    "                             iterations=100,\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "experiment_name = 'fraud-classification'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "#local_run = experiment.submit(automl_config, show_output = True)\n",
    "remote_run = experiment.submit(automl_config, show_output = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(remote_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(remote_run.get_output())\n",
    "best_run, fitted_model = remote_run.get_output()\n",
    "print(best_run)\n",
    "print(fitted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.automl.automlexplainer import retrieve_model_explanation\n",
    "\n",
    "shap_values, expected_values, overall_summary, overall_imp, per_class_summary, per_class_imp = \\\n",
    "    retrieve_model_explanation(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(overall_summary)\n",
    "print(overall_imp)\n",
    "print(per_class_summary)\n",
    "print(per_class_imp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(expected_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
